<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="google-site-verification" content="IOvSWj6LNI-7ZLD66WZIejP9vzeQo1poOO1J5MYr-TU" />
<title>Longteng  Guo</title>
<meta name="description" content="A practitioner of Computer Vision and Deep Learning.">

<!-- Open Graph -->

<meta property="og:site_name" content="A practitioner of Computer Vision and Deep Learning." />
<meta property="og:type" content="object" />
<meta property="og:title" content="" />
<meta property="og:url" content="/" />
<meta property="og:description" content="about" />
<meta property="og:image" content="" />


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/">

<!-- Theming-->

  <script src="/assets/js/theme.js"></script>





  <!-- Panelbear Analytics - We respect your privacy -->
  <script async src="https://cdn.panelbear.com/analytics.js?site=Ibm8AyMybeI"></script>
  <script>
    window.panelbear = window.panelbear || function() { (window.panelbear.q = window.panelbear.q || []).push(arguments); };
    panelbear('config', { site: 'Ibm8AyMybeI' });
  </script>

    

  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
        <!-- Social Icons -->
        <div class="row ml-1 ml-sm-0">
          <span class="contact-icon text-center">
  <a href="mailto:%6C%6F%6E%67%74%65%6E%67.%67%75%6F@%6E%6C%70%72.%69%61.%61%63.%63%6E"><i class="fas fa-envelope"></i></a>
  
  <a href="https://scholar.google.com/citations?user=OaGRHWYAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
  
  
  <a href="https://github.com/ltguo19" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
  <a href="https://www.linkedin.com/in/龙腾-郭-823753ba" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
  
  
  
  
  
  
</span>

        </div>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              About
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/awards/">
                Awards
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Publications
                
              </a>
          </li>
          
          
          
          
          
          
	  <!-- CV -->
<!--           <li class="nav-item ">
	    <a class="nav-link" target="_blank" href="/assets/pdf/Zhuoran_s_CV.pdf">CV</a>
          </li> -->
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Longteng   Guo </span> (郭龙腾) 
    </h1>
     <p class="desc">Ph.D. Candidate | Computer Vision</p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/prof_pic.jpg">
      
      
    </div>
    

    <div class="clearfix">
      <p>I am a Ph.D. student at <a href="http://www.ia.cas.cn/">National Laboratory of Pattern Recognition, Chinese Academy of Sciences</a>, advised by Prof. Jing Liu and Prof. Hanqing Lu. I mainly research on computer vision, including vision &amp; language, image retrieval, and semantic segmentation.</p>

<p>Before starting graduate study, I earned my B.Eng and B.Econ from <a href="http://www.xjtu.edu.cn/">Xi’an Jiaotong University</a> in 2016.</p>


    </div>

    
      <div class="news">
  <h2>News</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">Jun, 2020</th>
          <td>
            
              Our team is the winner of CVPR 2020 VATEX Video Captioning Challenge. ✨


            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Apr, 2020</th>
          <td>
            
              One paper is accepted by IJCAI 2020.


            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Feb, 2020</th>
          <td>
            
              One paper is accepted by CVPR 2020.


            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Oct, 2019</th>
          <td>
            
              Our team achieves the second place in ICCV 2019 VATEX Video Captioning Challenge.


            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h2>Selected Publications</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/nag.jpg">
  
  </div>

  <div id="nag" class="col-sm-8">
    
      <div class="title">Fast Sequence Generation with Multi-Agent Reinforcement Learning</div>
      <div class="author">
        
          
          
          
          
          

          
            
              
                <em>Guo, Longteng</em>,
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="http://www.nlpr.ia.ac.cn/iva/liujing/" target="_blank">Liu, Jing</a>,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Zhu, Xinxin,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="http://www.nlpr.ia.ac.cn/iva/luhanqing20150604/index.html" target="_blank">Lu, Hanqing</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Journal</em>
      
      
        2021
      
      
        (Under Review)
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2101.09698" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Autoregressive sequence Generation models have achieved state-of-the-art performance in areas like machine translation and image captioning. These models are autoregressive in that they generate each word by conditioning on previously generated words, which leads to heavy latency during inference. Recently, non-autoregressive decoding has been proposed in machine translation to speed up the inference time by generating all words in parallel. Typically, these models use the word-level cross-entropy loss to optimize each word independently. However, such a learning process fails to consider the sentence-level consistency, thus resulting in inferior generation quality of these non-autoregressive models. In this paper, we propose a simple and efficient model for Non-Autoregressive sequence Generation (NAG) with a novel training paradigm: Counterfactuals-critical Multi-Agent Learning (CMAL). CMAL formulates NAG as a multi-agent reinforcement learning system where element positions in the target sequence are viewed as agents that learn to cooperatively maximize a sentence-level reward. On MSCOCO image captioning benchmark, our NAG method achieves a performance comparable to state-of-the-art autoregressive models, while brings 13.9x decoding speedup. On WMT14 EN-DE machine translation dataset, our method outperforms cross-entropy trained baseline by 6.0 BLEU points while achieves the greatest decoding speedup of 17.46x.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/cptr.jpg">
  
  </div>

  <div id="cptr" class="col-sm-8">
    
      <div class="title">CPTR: Full Transformer Network for Image Captioning</div>
      <div class="author">
        
          
          
          
          
          
            
              
            
          

          
            
              
                
                  Liu, Wei,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Chen, Sihan,
                
              
            
          
        
          
          
          
          
          

          
            
              
                <em>Guo, Longteng</em>,
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Zhu, Xinxin,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="http://www.nlpr.ia.ac.cn/iva/liujing/" target="_blank">Liu, Jing</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In arxiv</em>
      
      
        2021
      
      
        (Under Review)
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2101.10804.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this paper, we consider the image captioning task from a new sequence-to-sequence prediction perspective and propose Caption TransformeR (CPTR) which takes the sequentialized raw images as the input to Transformer. Compared to the "CNN+Transformer" design paradigm, our model can model global context at every encoder layer from the beginning and is totally convolution-free. Extensive experiments demonstrate the effectiveness of the proposed model and we surpass the conventional "CNN+Transformer" methods on the MSCOCO dataset. Besides, we provide detailed visualizations of the self-attention between patches in the encoder and the "words-to-patches" attention in the decoder thanks to the full Transformer architecture.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/autocap.jpg">
  
  </div>

  <div id="autocap" class="col-sm-8">
    
      <div class="title">AutoCaption: Image Captioning with Neural Architecture Search</div>
      <div class="author">
        
          
          
          
          
          

          
            
              
                
                  Zhu, Xinxin,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Wang, Weining,
                
              
            
          
        
          
          
          
          
          

          
            
              
                <em>Guo, Longteng</em>,
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="http://www.nlpr.ia.ac.cn/iva/liujing/" target="_blank">Liu, Jing</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In arxiv</em>
      
      
        2020
      
      
        (Under Review)
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2012.09742" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Image captioning transforms complex visual information into abstract natural language for representation, which can help computers understanding the world quickly. However, due to the complexity of the real environment, it needs to identify key objects and realize their connections, and further generate natural language. The whole process involves a visual understanding module and a language generation module, which brings more challenges to the design of deep neural networks than other tasks. Neural Architecture Search (NAS) has shown its important role in a variety of image recognition tasks. Besides, RNN plays an essential role in the image captioning task. We introduce a AutoCaption method to better design the decoder module of the image captioning where we use the NAS to design the decoder module called AutoRNN automatically. We use the reinforcement learning method based on shared parameters for automatic design the AutoRNN efficiently. The search space of the AutoCaption includes connections between the layers and the operations in layers both, and it can make AutoRNN express more architectures. In particular, RNN is equivalent to a subset of our search space. Experiments on the MSCOCO datasets show that our AutoCaption model can achieve better performance than traditional hand-design methods. Our AutoCaption obtains the best published CIDEr performance of 135.8% on COCO Karpathy test split. When further using ensemble technology, CIDEr is boosted up to 139.5%.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/naic.jpg">
  
  </div>

  <div id="naic" class="col-sm-8">
    
      <div class="title">Non-Autoregressive Image Captioning with Counterfactuals-Critical Multi-Agent Learning</div>
      <div class="author">
        
          
          
          
          
          

          
            
              
                <em>Guo, Longteng</em>,
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="http://www.nlpr.ia.ac.cn/iva/liujing/" target="_blank">Liu, Jing</a>,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Zhu, Xinxin,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  He, Xingjian,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="https://scholar.google.com/citations?user=dwCuzo0AAAAJ" target="_blank">Jiang, Jie</a>,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="http://www.nlpr.ia.ac.cn/iva/luhanqing20150604/index.html" target="_blank">Lu, Hanqing</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In IJCAI</em>
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://www.ijcai.org/Proceedings/2020/0107.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Most image captioning models are autoregressive, i.e. they generate each word by conditioning on previously generated words, which leads to heavy latency during inference. Recently, non-autoregressive decoding has been proposed in machine translation to speed up the inference time by generating all words in parallel. Typically, these models use the word-level cross-entropy loss to optimize each word independently. However, such a learning process fails to consider the sentence-level consistency, thus resulting in inferior generation quality of these non-autoregressive models. In this paper, we propose a Non-Autoregressive Image Captioning (NAIC) model with a novel training paradigm: Counterfactuals-critical Multi-Agent Learning (CMAL). CMAL formulates NAIC as a multi-agent reinforcement learning system where positions in the target sequence are viewed as agents that learn to cooperatively maximize a sentence-level reward. Besides, we propose to utilize massive unlabeled images to boost captioning performance. Extensive experiments on MSCOCO image captioning benchmark show that our NAIC model achieves a performance comparable to state-of-the-art autoregressive models, while brings 13.9x decoding speedup.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/ngsan.jpg">
  
  </div>

  <div id="ngsan" class="col-sm-8">
    
      <div class="title">Normalized and Geometry-Aware Self-Attention Network for Image Captioning</div>
      <div class="author">
        
          
          
          
          
          

          
            
              
                <em>Guo, Longteng</em>,
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="http://www.nlpr.ia.ac.cn/iva/liujing/" target="_blank">Liu, Jing</a>,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Zhu, Xinxin,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Yao, Peng,
                
              
            
          
        
          
          
          
          
          
            
              
            
          

          
            
              
                
                  Lu, Shichen,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="http://www.nlpr.ia.ac.cn/iva/luhanqing20150604/index.html" target="_blank">Lu, Hanqing</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In CVPR</em>
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Normalized_and_Geometry-Aware_Self-Attention_Network_for_Image_Captioning_CVPR_2020_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Self-attention (SA) network has shown profound value in image captioning. In this paper, we improve SA from two aspects to promote the performance of image captioning. First, we propose Normalized Self-Attention (NSA), a reparameterization of SA that brings the benefits of normalization inside SA. While normalization is previously only applied outside SA, we introduce a novel normalization method and demonstrate that it is both possible and beneficial to perform it on the hidden activations inside SA. Second, to compensate for the major limit of Transformer that it fails to model the geometry structure of the input objects, we propose a class of Geometry-aware Self-Attention (GSA) that extends SA to explicitly and efficiently consider the relative geometry relations between the objects in the image. To construct our image captioning model, we combine the two modules and apply it to the vanilla self-attention network. We extensively evaluate our proposals on MS-COCO image captioning dataset and superior results are achieved when comparing to state-of-the-art approaches. Further experiments on three challenging tasks, i.e. video captioning, machine translation, and visual question answering, show the generality of our methods.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/ruminate.jpg">
  
  </div>

  <div id="ruminate" class="col-sm-8">
    
      <div class="title">Show, Tell, and Polish: Ruminant Decoding for Image Captioning</div>
      <div class="author">
        
          
          
          
          
          

          
            
              
                <em>Guo, Longteng</em>,
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="http://www.nlpr.ia.ac.cn/iva/liujing/" target="_blank">Liu, Jing</a>,
                
              
            
          
        
          
          
          
          
          
            
              
            
          

          
            
              
                
                  Lu, Shichen,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="http://www.nlpr.ia.ac.cn/iva/luhanqing20150604/index.html" target="_blank">Lu, Hanqing</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Transactions on Multimedia</em>
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://ieeexplore.ieee.org/abstract/document/8890876" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The encoder-decoder framework has been the base of popular image captioning models, which typically predicts the target sentence based on the encoded source image one word at a time in sequence. However, such a single-pass decoding framework encounters two problems. First, mistakes in the predicted words cannot be corrected and may propagate to the entire sentence. Second, because the single-pass decoder cannot access the following un-generated words, it can only perform local planning to choose every single word according to the preceding words, while lacks the global planning ability as for maintaining the semantic consistency and fluency of the whole sentence. In order to address the above two problems, in this work, we design a ruminant captioning framework which contains an image encoder, a base decoder, and a ruminant decoder. Specifically, the outputs of the former/base decoder are utilized as the global information to guide the words prediction of the latter/ruminant decoder, in an attempt to mimic human polishing process. We enable jointly training of the whole framework and overcome the non-differential problem of discrete words by designing a novel reinforcement learning based optimization algorithm. Experiments on two datasets (MS COCO and Flickr30 k) demonstrate that our ruminant decoding method can bring significant improvements over traditional single-pass decoding based models and achieves state-of-the-art performance.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/mscap.jpg">
  
  </div>

  <div id="mscap" class="col-sm-8">
    
      <div class="title">MSCap: Multi-Style Image Captioning With Unpaired Stylized Text</div>
      <div class="author">
        
          
          
          
          
          

          
            
              
                <em>Guo, Longteng</em>,
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="http://www.nlpr.ia.ac.cn/iva/liujing/" target="_blank">Liu, Jing</a>,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Yao, Peng,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Li, Jiangwei,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="http://www.nlpr.ia.ac.cn/iva/luhanqing20150604/index.html" target="_blank">Lu, Hanqing</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In CVPR</em>
      
      
        2019
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Guo_MSCap_Multi-Style_Image_Captioning_With_Unpaired_Stylized_Text_CVPR_2019_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this paper, we propose an adversarial learning network for the task of multi-style image captioning (MSCap) with a standard factual image caption dataset and a multi-stylized language corpus without paired images. How to learn a single model for multi-stylized image captioning with unpaired data is a challenging and necessary task, whereas rarely studied in previous works. The proposed framework mainly includes four contributive modules following a typical image encoder. First, a style dependent caption generator to output a sentence conditioned on an encoded image and a specified style. Second, a caption discriminator is presented to distinguish the input sentence to be real or not. The discriminator and the generator are trained in an adversarial manner to enable more natural and human-like captions. Third, a style classifier is employed to discriminate the specific style of the input sentence. Besides, a back-translation module is designed to enforce the generated stylized captions are visually grounded, with the intuition of the cycle consistency for factual caption and stylized caption. We enable an end-to-end optimization of the whole model with differentiable softmax approximation. At last, we conduct comprehensive experiments using a combined dataset containing four caption styles to demonstrate the outstanding performance of our proposed method.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/vsua.jpg">
  
  </div>

  <div id="vsua" class="col-sm-8">
    
      <div class="title">Aligning Linguistic Words and Visual Semantic Units for Image Captioning</div>
      <div class="author">
        
          
          
          
          
          

          
            
              
                <em>Guo, Longteng</em>,
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="http://www.nlpr.ia.ac.cn/iva/liujing/" target="_blank">Liu, Jing</a>,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="https://scholar.google.com/citations?user=ByBLlEwAAAAJ" target="_blank">Tang, Jinhui</a>,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Li, Jiangwei,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Luo, Wei,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="http://www.nlpr.ia.ac.cn/iva/luhanqing20150604/index.html" target="_blank">Lu, Hanqing</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In ACM MM</em>
      
      
        2019
      
      
        <b>Oral</b>
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://dl.acm.org/doi/pdf/10.1145/3343031.3350943" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/ltguo19/VSUA-Captioning" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Image captioning attempts to generate a sentence composed of several linguistic words, which are used to describe objects, attributes, and interactions in an image, denoted as visual semantic units in this paper. Based on this view, we propose to explicitly model the object interactions in semantics and geometry based on Graph Convolutional Networks (GCNs), and fully exploit the alignment between linguistic words and visual semantic units for image captioning. Particularly, we construct a semantic graph and a geometry graph, where each node corresponds to a visual semantic unit, i.e., an object, an attribute, or a semantic (geometrical) interaction between two objects. Accordingly, the semantic (geometrical) context-aware embeddings for each unit are obtained through the corresponding GCN learning processers. At each time step, a context gated attention module takes as inputs the embeddings of the visual semantic units and hierarchically align the current word with these units by first deciding which type of visual semantic unit (object, attribute, or interaction) the current word is about, and then finding the most correlated visual semantic units under this type. Extensive experiments are conducted on the challenging MS-COCO image captioning dataset, and superior results are reported when comparing to state-of-the-art approaches.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/sketch.jpg">
  
  </div>

  <div id="sketch" class="col-sm-8">
    
      <div class="title">Sketch-Based Image Retrieval Using Generative Adversarial Networks</div>
      <div class="author">
        
          
          
          
          
          

          
            
              
                <em>Guo, Longteng</em>,
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="http://www.nlpr.ia.ac.cn/iva/liujing/" target="_blank">Liu, Jing</a>,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Wang, Yuhang,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Luo, Zhonghua,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Wen, Wei,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="http://www.nlpr.ia.ac.cn/iva/luhanqing20150604/index.html" target="_blank">Lu, Hanqing</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In ACM MM</em>
      
      
        2017
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://dl.acm.org/doi/pdf/10.1145/3123266.3127939" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>For sketch-based image retrieval (SBIR), we propose a generative adversarial network trained on a large number of sketches and their corresponding real images. To imitate human search process, we attempt to match candidate images with theimaginary image in user single s mind instead of the sketch query, i.e., not only the shape information of sketches but their possible content information are considered in SBIR. Specifically, a conditional generative adversarial network (cGAN) is employed to enrich the content information of sketches and recover the imaginary images, and two VGG-based encoders, which work on real and imaginary images respectively, are used to constrain their perceptual consistency from the view of feature representations. During SBIR, we first generate an imaginary image from a given sketch via cGAN, and then take the output of the learned encoder for imaginary images as the feature of the query sketch. Finally, we build an interactive SBIR system that shows encouraging performance.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/localglobalcontext.jpg">
  
  </div>

  <div id="localglobalcontext" class="col-sm-8">
    
      <div class="title">Modeling Local and Global Contexts for Image Captioning</div>
      <div class="author">
        
          
          
          
          
          

          
            
              
                
                  Yao, Peng,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Li, Jiangyun,
                
              
            
          
        
          
          
          
          
          

          
            
              
                <em>Guo, Longteng</em>,
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="http://www.nlpr.ia.ac.cn/iva/liujing/" target="_blank">Liu, Jing</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In ICME</em>
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=9102935" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Image captioning aims to first observe an image, most notably the involved objects that are highly context-dependent, and then depict it with a natural description. However, most of the current models solely use the isolated objects vectors as image representations, ignoring the contexts among them. In this paper, we introduce a Local-Global Context (LGC) network, endowing the independent object features with shortrange perception (local contexts) and long-range dependence (global contexts). LGC network can be viewed as feature refiner, much beneficial to reason the novel objects and verbal words for the caption decoder. The local contexts are modeled with 1-D group convolution on adjacent objects, strengthening the local connections. Still further, self-attention mechanism is utilized to model the global contexts by correlating all the local contexts. Extensive experiments on MSCOCO dataset demonstrate that LGC network can easily plug into almost any neural captioning models and significantly improve the model performance.</p>
    </div>
    
  </div>
</div>
</li></ol>
</div>

    

    
      <div class="post">
  <h2>Honors & Awards</h2>
  <article>
    <ul>
  <li>2020 CVPR VATEX Video Captioning Challenge, Both English and Chinese Tracks (<b><a href="https://eric-xw.github.io/vatex-website/captioning_2020.html">First Place</a></b>)</li>
  <li>2019 ICCV VATEX Video Captioning Challenge (<b><a href="https://eric-xw.github.io/vatex-website/captioning.html">Second Place &amp; Outstanding Method Award</a></b>)</li>
  <li>2017 ICCV Places Challenge, Scene Parsing Track (<b><a href="http://news.sciencenet.cn/htmlnews/2017/11/393356.shtm">First Place</a></b>)</li>
  <li>2017 AI Challenger, Chinese Image Captioning Track (<b><a href="https://baike.baidu.com/item/AI%20Challenger/22448232">Champion of Double Week Competition</a></b>)</li>
  <li>2020 National Scholarship for Ph.D. Students</li>
  <li>2016 Outstanding Graduates Award, Xi’an Jiaotong University</li>
</ul>

  </article>
</div>

    

    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    &copy; Copyright 2021 Longteng  Guo.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.

    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
