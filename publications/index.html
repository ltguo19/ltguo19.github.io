<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="google-site-verification" content="IOvSWj6LNI-7ZLD66WZIejP9vzeQo1poOO1J5MYr-TU" />
<title>Longteng  Guo | Publications</title>
<meta name="description" content="A practitioner of Computer Vision and Deep Learning.">

<!-- Open Graph -->

<meta property="og:site_name" content="A practitioner of Computer Vision and Deep Learning." />
<meta property="og:type" content="object" />
<meta property="og:title" content="" />
<meta property="og:url" content="/publications/" />
<meta property="og:description" content="Publications" />
<meta property="og:image" content="" />


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">

<!-- Theming-->

  <script src="/assets/js/theme.js"></script>





  <!-- Panelbear Analytics - We respect your privacy -->
  <script async src="https://cdn.panelbear.com/analytics.js?site=Ibm8AyMybeI"></script>
  <script>
    window.panelbear = window.panelbear || function() { (window.panelbear.q = window.panelbear.q || []).push(arguments); };
    panelbear('config', { site: 'Ibm8AyMybeI' });
  </script>

    

  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Longteng</span>   Guo
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/awards/">
                Awards
                
              </a>
          </li>
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                Publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          
          
          
	  <!-- CV -->
<!--           <li class="nav-item ">
	    <a class="nav-link" target="_blank" href="/assets/pdf/Zhuoran_s_CV.pdf">CV</a>
          </li> -->
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">Publications </h1>
    <p class="post-description"></p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/nag.jpg" />
  
  </div>

  <div id="nag" class="col-sm-8">
    
      <div class="title">Fast Sequence Generation with Multi-Agent Reinforcement Learning</div>
      <div class="author">
        
          
          
          
          
          

          
            
              
                <em>Guo, Longteng</em>,
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="http://www.nlpr.ia.ac.cn/iva/liujing/" target="_blank">Liu, Jing</a>,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Zhu, Xinxin,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="http://www.nlpr.ia.ac.cn/iva/luhanqing20150604/index.html" target="_blank">Lu, Hanqing</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Journal</em>
      
      
        2021
      
      
        (Under Review)
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2101.09698" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Autoregressive sequence Generation models have achieved state-of-the-art performance in areas like machine translation and image captioning. These models are autoregressive in that they generate each word by conditioning on previously generated words, which leads to heavy latency during inference. Recently, non-autoregressive decoding has been proposed in machine translation to speed up the inference time by generating all words in parallel. Typically, these models use the word-level cross-entropy loss to optimize each word independently. However, such a learning process fails to consider the sentence-level consistency, thus resulting in inferior generation quality of these non-autoregressive models. In this paper, we propose a simple and efficient model for Non-Autoregressive sequence Generation (NAG) with a novel training paradigm: Counterfactuals-critical Multi-Agent Learning (CMAL). CMAL formulates NAG as a multi-agent reinforcement learning system where element positions in the target sequence are viewed as agents that learn to cooperatively maximize a sentence-level reward. On MSCOCO image captioning benchmark, our NAG method achieves a performance comparable to state-of-the-art autoregressive models, while brings 13.9x decoding speedup. On WMT14 EN-DE machine translation dataset, our method outperforms cross-entropy trained baseline by 6.0 BLEU points while achieves the greatest decoding speedup of 17.46x.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/cptr.jpg" />
  
  </div>

  <div id="cptr" class="col-sm-8">
    
      <div class="title">CPTR: Full Transformer Network for Image Captioning</div>
      <div class="author">
        
          
          
          
          
          
            
              
            
          

          
            
              
                
                  Liu, Wei,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Chen, Sihan,
                
              
            
          
        
          
          
          
          
          

          
            
              
                <em>Guo, Longteng</em>,
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Zhu, Xinxin,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="http://www.nlpr.ia.ac.cn/iva/liujing/" target="_blank">Liu, Jing</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In arxiv</em>
      
      
        2021
      
      
        (Under Review)
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2101.10804.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this paper, we consider the image captioning task from a new sequence-to-sequence prediction perspective and propose Caption TransformeR (CPTR) which takes the sequentialized raw images as the input to Transformer. Compared to the "CNN+Transformer" design paradigm, our model can model global context at every encoder layer from the beginning and is totally convolution-free. Extensive experiments demonstrate the effectiveness of the proposed model and we surpass the conventional "CNN+Transformer" methods on the MSCOCO dataset. Besides, we provide detailed visualizations of the self-attention between patches in the encoder and the "words-to-patches" attention in the decoder thanks to the full Transformer architecture.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/autocap.jpg" />
  
  </div>

  <div id="autocap" class="col-sm-8">
    
      <div class="title">AutoCaption: Image Captioning with Neural Architecture Search</div>
      <div class="author">
        
          
          
          
          
          

          
            
              
                
                  Zhu, Xinxin,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Wang, Weining,
                
              
            
          
        
          
          
          
          
          

          
            
              
                <em>Guo, Longteng</em>,
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="http://www.nlpr.ia.ac.cn/iva/liujing/" target="_blank">Liu, Jing</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In arxiv</em>
      
      
        2020
      
      
        (Under Review)
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2012.09742" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Image captioning transforms complex visual information into abstract natural language for representation, which can help computers understanding the world quickly. However, due to the complexity of the real environment, it needs to identify key objects and realize their connections, and further generate natural language. The whole process involves a visual understanding module and a language generation module, which brings more challenges to the design of deep neural networks than other tasks. Neural Architecture Search (NAS) has shown its important role in a variety of image recognition tasks. Besides, RNN plays an essential role in the image captioning task. We introduce a AutoCaption method to better design the decoder module of the image captioning where we use the NAS to design the decoder module called AutoRNN automatically. We use the reinforcement learning method based on shared parameters for automatic design the AutoRNN efficiently. The search space of the AutoCaption includes connections between the layers and the operations in layers both, and it can make AutoRNN express more architectures. In particular, RNN is equivalent to a subset of our search space. Experiments on the MSCOCO datasets show that our AutoCaption model can achieve better performance than traditional hand-design methods. Our AutoCaption obtains the best published CIDEr performance of 135.8% on COCO Karpathy test split. When further using ensemble technology, CIDEr is boosted up to 139.5%.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/naic.jpg" />
  
  </div>

  <div id="naic" class="col-sm-8">
    
      <div class="title">Non-Autoregressive Image Captioning with Counterfactuals-Critical Multi-Agent Learning</div>
      <div class="author">
        
          
          
          
          
          

          
            
              
                <em>Guo, Longteng</em>,
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="http://www.nlpr.ia.ac.cn/iva/liujing/" target="_blank">Liu, Jing</a>,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Zhu, Xinxin,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  He, Xingjian,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="https://scholar.google.com/citations?user=dwCuzo0AAAAJ" target="_blank">Jiang, Jie</a>,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="http://www.nlpr.ia.ac.cn/iva/luhanqing20150604/index.html" target="_blank">Lu, Hanqing</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In IJCAI</em>
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://www.ijcai.org/Proceedings/2020/0107.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Most image captioning models are autoregressive, i.e. they generate each word by conditioning on previously generated words, which leads to heavy latency during inference. Recently, non-autoregressive decoding has been proposed in machine translation to speed up the inference time by generating all words in parallel. Typically, these models use the word-level cross-entropy loss to optimize each word independently. However, such a learning process fails to consider the sentence-level consistency, thus resulting in inferior generation quality of these non-autoregressive models. In this paper, we propose a Non-Autoregressive Image Captioning (NAIC) model with a novel training paradigm: Counterfactuals-critical Multi-Agent Learning (CMAL). CMAL formulates NAIC as a multi-agent reinforcement learning system where positions in the target sequence are viewed as agents that learn to cooperatively maximize a sentence-level reward. Besides, we propose to utilize massive unlabeled images to boost captioning performance. Extensive experiments on MSCOCO image captioning benchmark show that our NAIC model achieves a performance comparable to state-of-the-art autoregressive models, while brings 13.9x decoding speedup.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/ngsan.jpg" />
  
  </div>

  <div id="ngsan" class="col-sm-8">
    
      <div class="title">Normalized and Geometry-Aware Self-Attention Network for Image Captioning</div>
      <div class="author">
        
          
          
          
          
          

          
            
              
                <em>Guo, Longteng</em>,
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="http://www.nlpr.ia.ac.cn/iva/liujing/" target="_blank">Liu, Jing</a>,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Zhu, Xinxin,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Yao, Peng,
                
              
            
          
        
          
          
          
          
          
            
              
            
          

          
            
              
                
                  Lu, Shichen,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="http://www.nlpr.ia.ac.cn/iva/luhanqing20150604/index.html" target="_blank">Lu, Hanqing</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In CVPR</em>
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Normalized_and_Geometry-Aware_Self-Attention_Network_for_Image_Captioning_CVPR_2020_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Self-attention (SA) network has shown profound value in image captioning. In this paper, we improve SA from two aspects to promote the performance of image captioning. First, we propose Normalized Self-Attention (NSA), a reparameterization of SA that brings the benefits of normalization inside SA. While normalization is previously only applied outside SA, we introduce a novel normalization method and demonstrate that it is both possible and beneficial to perform it on the hidden activations inside SA. Second, to compensate for the major limit of Transformer that it fails to model the geometry structure of the input objects, we propose a class of Geometry-aware Self-Attention (GSA) that extends SA to explicitly and efficiently consider the relative geometry relations between the objects in the image. To construct our image captioning model, we combine the two modules and apply it to the vanilla self-attention network. We extensively evaluate our proposals on MS-COCO image captioning dataset and superior results are achieved when comparing to state-of-the-art approaches. Further experiments on three challenging tasks, i.e. video captioning, machine translation, and visual question answering, show the generality of our methods.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/ruminate.jpg" />
  
  </div>

  <div id="ruminate" class="col-sm-8">
    
      <div class="title">Show, Tell, and Polish: Ruminant Decoding for Image Captioning</div>
      <div class="author">
        
          
          
          
          
          

          
            
              
                <em>Guo, Longteng</em>,
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="http://www.nlpr.ia.ac.cn/iva/liujing/" target="_blank">Liu, Jing</a>,
                
              
            
          
        
          
          
          
          
          
            
              
            
          

          
            
              
                
                  Lu, Shichen,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="http://www.nlpr.ia.ac.cn/iva/luhanqing20150604/index.html" target="_blank">Lu, Hanqing</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Transactions on Multimedia</em>
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://ieeexplore.ieee.org/abstract/document/8890876" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The encoder-decoder framework has been the base of popular image captioning models, which typically predicts the target sentence based on the encoded source image one word at a time in sequence. However, such a single-pass decoding framework encounters two problems. First, mistakes in the predicted words cannot be corrected and may propagate to the entire sentence. Second, because the single-pass decoder cannot access the following un-generated words, it can only perform local planning to choose every single word according to the preceding words, while lacks the global planning ability as for maintaining the semantic consistency and fluency of the whole sentence. In order to address the above two problems, in this work, we design a ruminant captioning framework which contains an image encoder, a base decoder, and a ruminant decoder. Specifically, the outputs of the former/base decoder are utilized as the global information to guide the words prediction of the latter/ruminant decoder, in an attempt to mimic human polishing process. We enable jointly training of the whole framework and overcome the non-differential problem of discrete words by designing a novel reinforcement learning based optimization algorithm. Experiments on two datasets (MS COCO and Flickr30 k) demonstrate that our ruminant decoding method can bring significant improvements over traditional single-pass decoding based models and achieves state-of-the-art performance.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/localglobalcontext.jpg" />
  
  </div>

  <div id="localglobalcontext" class="col-sm-8">
    
      <div class="title">Modeling Local and Global Contexts for Image Captioning</div>
      <div class="author">
        
          
          
          
          
          

          
            
              
                
                  Yao, Peng,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Li, Jiangyun,
                
              
            
          
        
          
          
          
          
          

          
            
              
                <em>Guo, Longteng</em>,
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="http://www.nlpr.ia.ac.cn/iva/liujing/" target="_blank">Liu, Jing</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In ICME</em>
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=9102935" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Image captioning aims to first observe an image, most notably the involved objects that are highly context-dependent, and then depict it with a natural description. However, most of the current models solely use the isolated objects vectors as image representations, ignoring the contexts among them. In this paper, we introduce a Local-Global Context (LGC) network, endowing the independent object features with shortrange perception (local contexts) and long-range dependence (global contexts). LGC network can be viewed as feature refiner, much beneficial to reason the novel objects and verbal words for the caption decoder. The local contexts are modeled with 1-D group convolution on adjacent objects, strengthening the local connections. Still further, self-attention mechanism is utilized to model the global contexts by correlating all the local contexts. Extensive experiments on MSCOCO dataset demonstrate that LGC network can easily plug into almost any neural captioning models and significantly improve the model performance.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/vatex2020.jpg" />
  
  </div>

  <div id="vatex2020" class="col-sm-8">
    
      <div class="title">Vatex Video Captioning Challenge 2020: Multi-View Features and Hybrid Reward Strategies for Video Captioning</div>
      <div class="author">
        
          
          
          
          
          

          
            
              
                <em>Guo, Longteng</em>,
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Zhu, Xinxin,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Yao, Peng,
                
              
            
          
        
          
          
          
          
          
            
              
            
          

          
            
              
                
                  Lu, Shichen,
                
              
            
          
        
          
          
          
          
          
            
              
            
          

          
            
              
                
                  Liu, Wei,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="http://www.nlpr.ia.ac.cn/iva/liujing/" target="_blank">Liu, Jing</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In CVPR Workshop</em>
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/1910.11102v4.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This report describes our solution for the VATEX Captioning Challenge 2020, which requires generating descriptions for the videos in both English and Chinese languages. We identified three crucial factors that improve the performance, namely: multi-view features, hybrid reward, and diverse ensemble. Based on our method of VATEX 2019 challenge, we achieved significant improvements this year with more advanced model architectures, combination of appearance and motion features, and careful hyper-parameters tuning. Our method achieves very competitive results on both of the Chinese and English video captioning tracks.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/mscap.jpg" />
  
  </div>

  <div id="mscap" class="col-sm-8">
    
      <div class="title">MSCap: Multi-Style Image Captioning With Unpaired Stylized Text</div>
      <div class="author">
        
          
          
          
          
          

          
            
              
                <em>Guo, Longteng</em>,
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="http://www.nlpr.ia.ac.cn/iva/liujing/" target="_blank">Liu, Jing</a>,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Yao, Peng,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Li, Jiangwei,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="http://www.nlpr.ia.ac.cn/iva/luhanqing20150604/index.html" target="_blank">Lu, Hanqing</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In CVPR</em>
      
      
        2019
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Guo_MSCap_Multi-Style_Image_Captioning_With_Unpaired_Stylized_Text_CVPR_2019_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this paper, we propose an adversarial learning network for the task of multi-style image captioning (MSCap) with a standard factual image caption dataset and a multi-stylized language corpus without paired images. How to learn a single model for multi-stylized image captioning with unpaired data is a challenging and necessary task, whereas rarely studied in previous works. The proposed framework mainly includes four contributive modules following a typical image encoder. First, a style dependent caption generator to output a sentence conditioned on an encoded image and a specified style. Second, a caption discriminator is presented to distinguish the input sentence to be real or not. The discriminator and the generator are trained in an adversarial manner to enable more natural and human-like captions. Third, a style classifier is employed to discriminate the specific style of the input sentence. Besides, a back-translation module is designed to enforce the generated stylized captions are visually grounded, with the intuition of the cycle consistency for factual caption and stylized caption. We enable an end-to-end optimization of the whole model with differentiable softmax approximation. At last, we conduct comprehensive experiments using a combined dataset containing four caption styles to demonstrate the outstanding performance of our proposed method.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/vsua.jpg" />
  
  </div>

  <div id="vsua" class="col-sm-8">
    
      <div class="title">Aligning Linguistic Words and Visual Semantic Units for Image Captioning</div>
      <div class="author">
        
          
          
          
          
          

          
            
              
                <em>Guo, Longteng</em>,
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="http://www.nlpr.ia.ac.cn/iva/liujing/" target="_blank">Liu, Jing</a>,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="https://scholar.google.com/citations?user=ByBLlEwAAAAJ" target="_blank">Tang, Jinhui</a>,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Li, Jiangwei,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Luo, Wei,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="http://www.nlpr.ia.ac.cn/iva/luhanqing20150604/index.html" target="_blank">Lu, Hanqing</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In ACM MM</em>
      
      
        2019
      
      
        <b>Oral</b>
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://dl.acm.org/doi/pdf/10.1145/3343031.3350943" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/ltguo19/VSUA-Captioning" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Image captioning attempts to generate a sentence composed of several linguistic words, which are used to describe objects, attributes, and interactions in an image, denoted as visual semantic units in this paper. Based on this view, we propose to explicitly model the object interactions in semantics and geometry based on Graph Convolutional Networks (GCNs), and fully exploit the alignment between linguistic words and visual semantic units for image captioning. Particularly, we construct a semantic graph and a geometry graph, where each node corresponds to a visual semantic unit, i.e., an object, an attribute, or a semantic (geometrical) interaction between two objects. Accordingly, the semantic (geometrical) context-aware embeddings for each unit are obtained through the corresponding GCN learning processers. At each time step, a context gated attention module takes as inputs the embeddings of the visual semantic units and hierarchically align the current word with these units by first deciding which type of visual semantic unit (object, attribute, or interaction) the current word is about, and then finding the most correlated visual semantic units under this type. Extensive experiments are conducted on the challenging MS-COCO image captioning dataset, and superior results are reported when comparing to state-of-the-art approaches.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/convatt.jpg" />
  
  </div>

  <div id="convatt" class="col-sm-8">
    
      <div class="title">Structure Preserving Convolutional Attention for Image Captioning</div>
      <div class="author">
        
          
          
          
          
          
            
              
            
          

          
            
              
                
                  Lu, Shichen,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Hu, Ruimin,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="http://www.nlpr.ia.ac.cn/iva/liujing/" target="_blank">Liu, Jing</a>,
                
              
            
          
        
          
          
          
          
          

          
            
              
                <em>Guo, Longteng</em>,
              
            
          
        
          
          
          
          
          

          
            
              
                
                  and Zheng, Fei
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Applied Sciences</em>
      
      
        2019
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://www.mdpi.com/2076-3417/9/14/2888" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In the task of image captioning, learning the attentive image regions is necessary to adaptively and precisely focus on the object semantics relevant to each decoded word. In this paper, we propose a convolutional attention module that can preserve the spatial structure of the image by performing the convolution operation directly on the 2D feature maps. The proposed attention mechanism contains two components: convolutional spatial attention and cross-channel attention, aiming to determine the intended regions to describe the image along the spatial and channel dimensions, respectively. Both of the two attentions are calculated at each decoding step. In order to preserve the spatial structure, instead of operating on the vector representation of each image grid, the two attention components are both computed directly on the entire feature maps with convolution operations. Experiments on two large-scale datasets (MSCOCO and Flickr30K) demonstrate the outstanding performance of our proposed method.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/boosttransformer.jpg" />
  
  </div>

  <div id="boosttransformer" class="col-sm-8">
    
      <div class="title">Boosted Transformer for Image Captioning</div>
      <div class="author">
        
          
          
          
          
          

          
            
              
                
                  Li, Jiangyun,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Yao, Peng,
                
              
            
          
        
          
          
          
          
          

          
            
              
                <em>Guo, Longteng</em>,
              
            
          
        
          
          
          
          
          

          
            
              
                
                  and Zhang, Weicun
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Applied Sciences</em>
      
      
        2019
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://www.mdpi.com/2076-3417/9/16/3260" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Image captioning attempts to generate a description given an image, usually taking Convolutional Neural Network as the encoder to extract the visual features and a sequence model, among which the self-attention mechanism has achieved advanced progress recently, as the decoder to generate descriptions. However, this predominant encoder-decoder architecture has some problems to be solved. On the encoder side, without the semantic concepts, the extracted visual features do not make full use of the image information. On the decoder side, the sequence self-attention only relies on word representations, lacking the guidance of visual information and easily influenced by the language prior. In this paper, we propose a novel boosted transformer model with two attention modules for the above-mentioned problems, i.e., &ldquo;Concept-Guided Attention&rdquo; (CGA) and &ldquo;Vision-Guided Attention&rdquo; (VGA). Our model utilizes CGA in the encoder, to obtain the boosted visual features by integrating the instance-level concepts into the visual features. In the decoder, we stack VGA, which uses the visual information as a bridge to model internal relationships among the sequences and can be an auxiliary module of sequence self-attention. Quantitative and qualitative results on the Microsoft COCO dataset demonstrate the better performance of our model than the state-of-the-art approaches.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/vatex2019.jpg" />
  
  </div>

  <div id="vatex2019" class="col-sm-8">
    
      <div class="title">Multi-View Features and Hybrid Reward Strategies for Vatex Video Captioning Challenge 2019</div>
      <div class="author">
        
          
          
          
          
          

          
            
              
                <em>Guo, Longteng</em>,
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Zhu, Xinxin,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Yao, Peng,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="http://www.nlpr.ia.ac.cn/iva/liujing/" target="_blank">Liu, Jing</a>,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  and <a href="http://www.nlpr.ia.ac.cn/iva/luhanqing20150604/index.html" target="_blank">Lu, Hanqing</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In ICCV Workshop</em>
      
      
        2019
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://arxiv.org/pdf/1910.11102v1.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This document describes our solution for the VATEX Captioning Challenge 2019, which requires generating descriptions for the videos in both English and Chinese languages. We identified three crucial factors that improve the performance, namely: multi-view features, hybrid reward, and diverse ensemble. Our method achieves the 2nd and the 3rd places on the Chinese and English video captioning tracks, respectively.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/wordgate.jpg" />
  
  </div>

  <div id="wordgate" class="col-sm-8">
    
      <div class="title">Image Captioning with Word Gate and Adaptive Self-Critical Learning</div>
      <div class="author">
        
          
          
          
          
          

          
            
              
                
                  Zhu, Xinxin,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Li, Lixiang,
                
              
            
          
        
          
          
          
          
          
            
              
                
                
          

          
            
              
                
                  <a href="http://www.nlpr.ia.ac.cn/iva/liujing/" target="_blank">Liu, Jing</a>,
                
              
            
          
        
          
          
          
          
          

          
            
              
                <em>Guo, Longteng</em>,
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Fang, Zhiwei,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  Peng, Haipeng,
                
              
            
          
        
          
          
          
          
          

          
            
              
                
                  and Niu, Xinxin
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Applied Sciences</em>
      
      
        2018
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://www.mdpi.com/2076-3417/8/6/909" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Although the policy-gradient methods for reinforcement learning have shown significant improvement in image captioning, how to achieve high performance during the reinforcement optimizing process is still not a simple task. There are at least two difficulties: (1) The large size of vocabulary leads to a large action space, which makes it difficult for the model to accurately predict the current word. (2) The large variance of gradient estimation in reinforcement learning usually causes severe instabilities in the training process. In this paper, we propose two innovations to boost the performance of self-critical sequence training (SCST). First, we modify the standard long short-term memory (LSTM)based decoder by introducing a gate function to reduce the search scope of the vocabulary for any given image, which is termed the word gate decoder. Second, instead of only considering current maximum actions greedily, we propose a stabilized gradient estimation method whose gradient variance is controlled by the difference between the sampling reward from the current model and the expectation of the historical reward. We conducted extensive experiments, and results showed that our method could accelerate the training process and increase the prediction accuracy. Our method was validated on MS COCO datasets and yielded state-of-the-art performance.</p>
    </div>
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    &copy; Copyright 2021 Longteng  Guo.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.

    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
